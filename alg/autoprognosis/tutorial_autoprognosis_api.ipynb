{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoPrognosis API Tutorial\n",
    "\n",
    "A demonstration for AP functionality and operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to use [Autoprognosis](https://arxiv.org/abs/1802.07207). We are using the UCI Spam dataset.\n",
    "\n",
    "See [installation instructions](../../doc/install.md) to install the dependencies.\n",
    "\n",
    "Load dataset and show the first five samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mojgan/opt/anaconda3/envs/mlEnv/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import initpath_ap\n",
    "initpath_ap.init_sys_path()\n",
    "import utilmlab\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "#df = load_breast_cancer()\n",
    "#X_ = pd.DataFrame(df.data)\n",
    "#Y_ = pd.DataFrame(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_= pd.read_feather('cardio_data/trained_df_noNan')\n",
    "X_.set_index('eid', inplace=True)\n",
    "Y_= pd.read_feather('cardio_data/trained_df_noNan_outcome')\n",
    "Y_.set_index('eid', inplace=True)\n",
    "#Y_=Y_['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a small random dataset\n",
    "df_all= X_.join(Y_)\n",
    "df_all=df_all.reindex(np.random.permutation(df_all.index))\n",
    "df_all=df_all[:20000]\n",
    "#df_all.to_csv('cardio_data/small_cardio_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all= df_all[['gender', 'age-0', 'average-sys-0', 'history-of-diabetes', 'hypertention-medication-0', 'smoker',\n",
    "                'average-BMI-0', 'outcome']]\n",
    "#df_all.to_csv('cardio_data/small_cardio_data_7_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_= df_all.drop(columns=['outcome'])\n",
    "Y_= df_all[['outcome']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the AutoPrognosis library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--it : total number of iterations for each fold or n-fold cross validation\n",
    "\n",
    "--cv : n for n-fold cross validation\n",
    "\n",
    "--nstage: size of pipeline: 0: auto (selects imputation when missing data is detected),\n",
    "        1: only classifiers, \n",
    "        2: feature processesing + clf, \n",
    "        3: imputers + feature processors and clf\n",
    "        \n",
    "--ensemble : include ensembles when fitting\n",
    "\n",
    "--modelindexes\n",
    "\n",
    "1 Random Forest,\n",
    "2 Gradient Boosting, \n",
    "3 XGBoost, \n",
    "4 Adaboost, \n",
    "5 Bagging, \n",
    "6 Bernoulli Naive Bayes, \n",
    "7 Gauss Naive Bayes, \n",
    "8 Multinomial Naive Bayes, \n",
    "9 Logistic Regression, \n",
    "10 Perceptron, \n",
    "11 Decision Trees, \n",
    "12 QDA, \n",
    "13 LDA, \n",
    "14 KNN, \n",
    "15 Linear SVM, \n",
    "16 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R[write to console]: Loading required package: missForest',\n",
       " '',\n",
       " 'R[write to console]: Loading required package: randomForest',\n",
       " '',\n",
       " 'R[write to console]: randomForest 4.6-14',\n",
       " '',\n",
       " 'R[write to console]: Type rfNews() to see new features/changes/bug fixes.',\n",
       " '',\n",
       " 'R[write to console]: Loading required package: foreach',\n",
       " '',\n",
       " 'R[write to console]: Loading required package: itertools',\n",
       " '',\n",
       " 'R[write to console]: Loading required package: iterators',\n",
       " '',\n",
       " 'R[write to console]: Loading required package: softImpute',\n",
       " '',\n",
       " 'R[write to console]: Loading required package: Matrix',\n",
       " '',\n",
       " 'R[write to console]: Loaded softImpute 1.4',\n",
       " '',\n",
       " '',\n",
       " 'Iteration number: 1 8s (8s) (78s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: 0.0',\n",
       " 'Iteration number: 2 10s (5s) (50s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.0',\n",
       " 'Iteration number: 3 13s (4s) (42s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.4043828032111159',\n",
       " 'Iteration number: 4 16s (4s) (40s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.0905468438642516',\n",
       " 'Iteration number: 5 20s (4s) (40s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.2896262039544037',\n",
       " 'Iteration number: 6 25s (4s) (42s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.4756191525618212',\n",
       " 'Iteration number: 7 32s (5s) (46s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.3668612636876916',\n",
       " 'Iteration number: 8 40s (5s) (50s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.2757162352596387',\n",
       " 'Iteration number: 9 48s (5s) (54s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.247838423921191',\n",
       " 'Iteration number: 10 58s (6s) (58s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.3380590976288338',\n",
       " 'Iteration number: 1 8s (8s) (77s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: 0.0',\n",
       " 'Iteration number: 2 13s (6s) (64s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.0000000000000007',\n",
       " 'Iteration number: 3 19s (6s) (62s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.027926917966276',\n",
       " 'Iteration number: 4 25s (6s) (63s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.1568287157225177',\n",
       " 'Iteration number: 5 30s (6s) (59s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.129540547547907',\n",
       " 'Iteration number: 6 33s (6s) (56s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.3539716466174487',\n",
       " 'Iteration number: 7 37s (5s) (53s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.2814682207228543',\n",
       " 'Iteration number: 8 41s (5s) (51s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], BO objective: -1.234732213813708',\n",
       " 'Iteration number: 9 44s (5s) (49s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.2519287987891055',\n",
       " 'Iteration number: 10 49s (5s) (49s), Current pipelines:  [[[ Random Forest ]]], [[[ Gradient Boosting ]]], [[[ XGBoost ]]], BO objective: -1.2871940063154894',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " \"HBox(children=(FloatProgress(value=0.0, description='BO progress', max=10.0, style=ProgressStyle(description_width='initial')), HTML(value='')))\",\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " \"HBox(children=(FloatProgress(value=0.0, description='BO progress', max=10.0, style=ProgressStyle(description_width='initial')), HTML(value='')))\",\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ AdaBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '[ Random Forest ]',\n",
       " '[ Gradient Boosting ]',\n",
       " '[ XGBoost ]',\n",
       " '',\n",
       " \"{'model_list': [<models.classifiers.GradientBoosting object at 0x1a2b652b50>], 'explained': '[ *Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function..* ]', 'image_name': None, 'classes': None, 'num_stages': 1, 'pipeline_stages': ['classifier'], 'name': '[ Gradient Boosting ]', 'analysis_mode': None, 'analysis_type': None}\",\n",
       " \"{'model_list': [<models.classifiers.XGboost object at 0x1a2b1ca1d0>], 'explained': '[ *GBoost is an open-source software library which provides the gradient boosting framework for C++, Java, Python, R, and Julia.* ]', 'image_name': None, 'classes': None, 'num_stages': 1, 'pipeline_stages': ['classifier'], 'name': '[ XGBoost ]', 'analysis_mode': None, 'analysis_type': None}\",\n",
       " \"{'model_list': [<models.classifiers.XGboost object at 0x1a2b1ca510>], 'explained': '[ *GBoost is an open-source software library which provides the gradient boosting framework for C++, Java, Python, R, and Julia.* ]', 'image_name': None, 'classes': None, 'num_stages': 1, 'pipeline_stages': ['classifier'], 'name': '[ XGBoost ]', 'analysis_mode': None, 'analysis_type': None}\",\n",
       " '  \\x1b[1mMat52.     \\x1b[0;0m  |               value  |  constraints  |  priors',\n",
       " '  \\x1b[1mvariance   \\x1b[0;0m  |  0.9830827858262251  |      +ve      |        ',\n",
       " '  \\x1b[1mlengthscale\\x1b[0;0m  |  14.583740136687428  |      +ve      |        ',\n",
       " '  \\x1b[1mMat52.     \\x1b[0;0m  |               value  |  constraints  |  priors',\n",
       " '  \\x1b[1mvariance   \\x1b[0;0m  |  1.9939146596238853  |      +ve      |        ',\n",
       " '  \\x1b[1mlengthscale\\x1b[0;0m  |   4.181531316796556  |      +ve      |        ',\n",
       " '  \\x1b[1mMat52.     \\x1b[0;0m  |              value  |  constraints  |  priors',\n",
       " '  \\x1b[1mvariance   \\x1b[0;0m  |  1.151556535780181  |      +ve      |        ',\n",
       " '  \\x1b[1mlengthscale\\x1b[0;0m  |  5.451282318483556  |      +ve      |        ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 autoprognosis.py -i ../../../AutoPrognosisThings/cardio_data/small_cardio_data_7_feature.csv\\\n",
    "--target outcome -o ../../../AutoPrognosisThings/outputs --it 10 --cv 2 --nstage 1 --ensemble 1 --modelindexes [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\r\n",
      "\r\n",
      "classifier      aucroc 0.723\r\n",
      "classifier      aucprc 0.068\r\n",
      "ensemble        aucroc 0.727\r\n",
      "ensemble        aucprc 0.068\r\n",
      "\r\n",
      "Report\r\n",
      "\r\n",
      "best score single pipeline (while fitting)    0.725\r\n",
      "model_names_single_pipeline                   [ Gradient Boosting ]\r\n",
      "best ensemble score (while fittng)            0.738\r\n",
      "ensemble_pipelines                            ['[ Gradient Boosting ]', '[ XGBoost ]', '[ XGBoost ]']\r\n",
      "ensemble_pipelines_weight                     [0.2413297261706806, 0.38940120006055917, 0.3692690737687602]\r\n",
      "optimisation_metric                           aucroc\r\n",
      "hyperparameter_properties                     [{'name': 'Gradient Boosting', 'hyperparameters': {'model': \"GradientBoostingClassifier(criterion='friedman_mse', init=None,\\n              learning_rate=0.5, loss='deviance', max_depth=1,\\n              max_features=None, max_leaf_nodes=None,\\n              min_impurity_decrease=0.0, min_impurity_split=None,\\n              min_samples_leaf=1, min_samples_split=2,\\n              min_weight_fraction_leaf=0.0, n_estimators=30,\\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\\n              warm_start=False)\"}}]\r\n",
      "acquisition_type                              LCB\r\n",
      "kernel_members                                0 ['Random Forest']\r\n",
      "kernel_members                                1 ['Gradient Boosting']\r\n",
      "kernel_members                                2 ['Adaboost', 'XGBoost']\r\n",
      "classes dataset                               [False, True]\r\n",
      "features                                      ['eid', 'gender', 'age-0', 'average-sys-0', 'history-of-diabetes', 'hypertention-medication-0', 'smoker', 'average-BMI-0']\r\n",
      "samples                                       20000\r\n",
      "(0, {'name': 'initial', 'aucroc': 0.6557980571789606})\r\n",
      "sort by aucroc\r\n",
      "# 62\r\n",
      "# 4\r\n",
      "\r\n",
      "Average performance per classifier (ignoring hyperparameters):\r\n",
      "\r\n",
      "  0 Gradient Boosting                                   20 0.692 0.056\r\n",
      "  1 XGBoost                                             12 0.680 0.053\r\n",
      "  2 Random Forest                                       20 0.673 0.049\r\n",
      "  3 AdaBoost                                             8 0.648 0.043\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python3 autoprognosis_report.py -i ../../../AutoPrognosisThings/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model by short simple python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric = 'aucprc'\n",
    "acquisition_type = 'MPI' # default and prefered is LCB but this generates excessive warnings, MPI is a good compromise.\n",
    "#I changed kernel_freq=100 and Gibbs_iter=100\n",
    "AP_mdl   = model.AutoPrognosis_Classifier(\n",
    "    metric=metric, CV=5, num_iter=3, kernel_freq=10, ensemble=False,\n",
    "    ensemble_size=3, Gibbs_iter=100, burn_in=50, num_components=3, \n",
    "    acquisition_type=acquisition_type, is_nan=False, use_imputer=False, use_preprocessor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: missForest\n",
      "\n",
      "R[write to console]: Loading required package: randomForest\n",
      "\n",
      "R[write to console]: randomForest 4.6-14\n",
      "\n",
      "R[write to console]: Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "R[write to console]: Loading required package: foreach\n",
      "\n",
      "R[write to console]: Loading required package: itertools\n",
      "\n",
      "R[write to console]: Loading required package: iterators\n",
      "\n",
      "R[write to console]: Loading required package: softImpute\n",
      "\n",
      "R[write to console]: Loading required package: Matrix\n",
      "\n",
      "R[write to console]: Loaded softImpute 1.4\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Gradient Boosting ]\n",
      "[ MultinomialNaiveBayes ]\n",
      "[ LinearSVM ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56673d36b6e040f2892e3d3eb113090b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='BO progress', max=3.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ NeuralNet ]\n",
      "[ MultinomialNaiveBayes ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration number: 1 43s (43s) (129s), Current pipelines:  [[[ NeuralNet ]]], [[[ MultinomialNaiveBayes ]]], [[[ DecisionTrees ]]], BO objective: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ DecisionTrees ]\n",
      "[ Gradient Boosting ]\n",
      "[ AdaBoost ]\n",
      "[ LDA ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration number: 2 90s (45s) (135s), Current pipelines:  [[[ Gradient Boosting ]]], [[[ AdaBoost ]]], [[[ LDA ]]], BO objective: -1.0000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ XGBoost ]\n",
      "[ Bagging ]\n",
      "[ LDA ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration number: 3 146s (49s) (146s), Current pipelines:  [[[ XGBoost ]]], [[[ Bagging ]]], [[[ LDA ]]], BO objective: -1.4142135623730951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**The best model is: **[ LDA ]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'initial', 'aucprc': 0.057172614799005705},\n",
       " {'aucprc': 0.042970511167461346,\n",
       "  'aucroc': 0.6587761425272423,\n",
       "  'name': '[ NeuralNet ]',\n",
       "  'cv': 5,\n",
       "  'iter': 0,\n",
       "  'component_idx': 0,\n",
       "  'hyperparameter_properties': [{'name': 'NeuralNet',\n",
       "    'hyperparameters': {'model': \"MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\\n       beta_2=0.999, early_stopping=False, epsilon=1e-08,\\n       hidden_layer_sizes=(50, 50), learning_rate='constant',\\n       learning_rate_init=0.001, max_iter=200, momentum=0.9,\\n       nesterovs_momentum=True, power_t=0.5, random_state=None,\\n       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\\n       verbose=False, warm_start=False)\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a25fe9550>'},\n",
       " {'aucprc': 0.026046956029114793,\n",
       "  'aucroc': 0.48341496651951343,\n",
       "  'name': '[ MultinomialNaiveBayes ]',\n",
       "  'cv': 5,\n",
       "  'iter': 0,\n",
       "  'component_idx': 1,\n",
       "  'hyperparameter_properties': [{'name': 'MultinomialNaiveBayes',\n",
       "    'hyperparameters': {'model': 'MultinomialNB(alpha=3.385277405518578, class_prior=None, fit_prior=True)'}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a25fdaf10>'},\n",
       " {'aucprc': 0.0,\n",
       "  'aucroc': 0.5,\n",
       "  'name': '[ DecisionTrees ]',\n",
       "  'cv': 5,\n",
       "  'iter': 0,\n",
       "  'component_idx': 2,\n",
       "  'hyperparameter_properties': [{'name': 'DecisionTrees',\n",
       "    'hyperparameters': {'model': \"DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best')\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a25ff2610>'},\n",
       " {'aucprc': 0.05638328870905825,\n",
       "  'aucroc': 0.6762716383996976,\n",
       "  'name': '[ Gradient Boosting ]',\n",
       "  'cv': 5,\n",
       "  'iter': 1,\n",
       "  'component_idx': 0,\n",
       "  'hyperparameter_properties': [{'name': 'Gradient Boosting',\n",
       "    'hyperparameters': {'model': \"GradientBoostingClassifier(criterion='friedman_mse', init=None,\\n              learning_rate=0.28105633052522483, loss='deviance',\\n              max_depth=2, max_features=None, max_leaf_nodes=None,\\n              min_impurity_decrease=0.0, min_impurity_split=None,\\n              min_samples_leaf=1, min_samples_split=2,\\n              min_weight_fraction_leaf=0.0, n_estimators=225,\\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\\n              warm_start=False)\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a1fba1f90>'},\n",
       " {'aucprc': 0.027890024965213433,\n",
       "  'aucroc': 0.5283823992040986,\n",
       "  'name': '[ AdaBoost ]',\n",
       "  'cv': 5,\n",
       "  'iter': 1,\n",
       "  'component_idx': 1,\n",
       "  'hyperparameter_properties': [{'name': 'AdaBoost',\n",
       "    'hyperparameters': {'model': \"AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=2.962420135668186, n_estimators=2305,\\n          random_state=None)\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a26093510>'},\n",
       " {'aucprc': 0.0624852889618833,\n",
       "  'aucroc': 0.7249708599993299,\n",
       "  'name': '[ LDA ]',\n",
       "  'cv': 5,\n",
       "  'iter': 1,\n",
       "  'component_idx': 2,\n",
       "  'hyperparameter_properties': [{'name': 'LDA',\n",
       "    'hyperparameters': {'model': \"LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\\n              solver='svd', store_covariance=False, tol=0.0001)\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a1fdca490>'},\n",
       " {'aucprc': 0.048220983902074646,\n",
       "  'aucroc': 0.646021813710327,\n",
       "  'name': '[ XGBoost ]',\n",
       "  'cv': 5,\n",
       "  'iter': 2,\n",
       "  'component_idx': 0,\n",
       "  'hyperparameter_properties': [{'name': 'XGBoost',\n",
       "    'hyperparameters': {'model': \"XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\\n       colsample_bynode=None, colsample_bytree=None, gamma=None,\\n       gpu_id=None, importance_type='gain', interaction_constraints=None,\\n       learning_rate=0.3691106981794623, max_delta_step=None, max_depth=5,\\n       min_child_weight=None, missing=nan, monotone_constraints=None,\\n       n_estimators=141, n_jobs=None, num_parallel_tree=None,\\n       objective='binary:logistic', random_state=None, reg_alpha=None,\\n       reg_lambda=None, scale_pos_weight=None, subsample=None,\\n       tree_method=None, validate_parameters=False, verbosity=None)\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a1fba1810>'},\n",
       " {'aucprc': 0.0572083668943544,\n",
       "  'aucroc': 0.6903383216422755,\n",
       "  'name': '[ Bagging ]',\n",
       "  'cv': 5,\n",
       "  'iter': 2,\n",
       "  'component_idx': 1,\n",
       "  'hyperparameter_properties': [{'name': 'Bagging',\n",
       "    'hyperparameters': {'model': 'BaggingClassifier(base_estimator=None, bootstrap=True,\\n         bootstrap_features=False, max_features=1.0,\\n         max_samples=0.1438421943852849, n_estimators=943, n_jobs=1,\\n         oob_score=False, random_state=None, verbose=0, warm_start=False)'}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a1fccca50>'},\n",
       " {'aucprc': 0.0624852889618833,\n",
       "  'aucroc': 0.7249708599993299,\n",
       "  'name': '[ LDA ]',\n",
       "  'cv': 5,\n",
       "  'iter': 2,\n",
       "  'component_idx': 2,\n",
       "  'hyperparameter_properties': [{'name': 'LDA',\n",
       "    'hyperparameters': {'model': \"LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\\n              solver='svd', store_covariance=False, tol=0.0001)\"}}],\n",
       "  'model': '<pipelines.basePipeline.basePipeline object at 0x1a1fdfce10>'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP_mdl.fit(X_, Y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ~~~First element in the output is the predictions of a single model, the second element is the prediction of the ensemble~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.98759929, 0.01240071],\n",
       "        [0.9912874 , 0.0087126 ],\n",
       "        [0.96629033, 0.03370967],\n",
       "        ...,\n",
       "        [0.95964161, 0.04035839],\n",
       "        [0.96429803, 0.03570197],\n",
       "        [0.98756443, 0.01243557]]),\n",
       " array([[0.98759929, 0.01240071],\n",
       "        [0.9912874 , 0.0087126 ],\n",
       "        [0.96629033, 0.03370967],\n",
       "        ...,\n",
       "        [0.95964161, 0.04035839],\n",
       "        [0.96429803, 0.03570197],\n",
       "        [0.98756443, 0.01243557]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP_mdl.predict(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance via multi-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_ens(X_, Y_, AP_mdl, n_folds=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_mdl.visualize_data(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\r",
       "***Ensemble Report***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**----------------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Rank0:   [ XGBoost ],   Ensemble weight: 0.337141036259983**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**----------------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_list': [<models.classifiers.XGboost object at 0x1a3466ae90>], 'explained': '[ *GBoost is an open-source software library which provides the gradient boosting framework for C++, Java, Python, R, and Julia.* ]', 'image_name': None, 'classes': None, 'num_stages': 1, 'pipeline_stages': ['classifier'], 'name': '[ XGBoost ]', 'analysis_mode': None, 'analysis_type': None}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**_____________________________________________**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "[ *GBoost is an open-source software library which provides the gradient boosting framework for C++, Java, Python, R, and Julia.* ]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Rank1:   [ AdaBoost ],   Ensemble weight: 0.33191778877869144**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**----------------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_list': [<models.classifiers.Adaboost object at 0x1a34668790>], 'explained': \"[ *AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers.* ]\", 'image_name': None, 'classes': None, 'num_stages': 1, 'pipeline_stages': ['classifier'], 'name': '[ AdaBoost ]', 'analysis_mode': None, 'analysis_type': None}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**_____________________________________________**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "[ *AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers.* ]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Rank2:   [ XGBoost ],   Ensemble weight: 0.33094117496132563**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**----------------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_list': [<models.classifiers.XGboost object at 0x1a376f1e90>], 'explained': '[ *GBoost is an open-source software library which provides the gradient boosting framework for C++, Java, Python, R, and Julia.* ]', 'image_name': None, 'classes': None, 'num_stages': 1, 'pipeline_stages': ['classifier'], 'name': '[ XGBoost ]', 'analysis_mode': None, 'analysis_type': None}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**_____________________________________________**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "[ *GBoost is an open-source software library which provides the gradient boosting framework for C++, Java, Python, R, and Julia.* ]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**----------------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "***Kernel Report***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Component 0**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Members: ['XGBoost', 'Gradient Boosting', 'Random Forest', 'Neural Network']**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1mMat52.     \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mvariance   \u001b[0;0m  |  0.9999990030869095  |      +ve      |        \n",
      "  \u001b[1mlengthscale\u001b[0;0m  |  0.9031481051397683  |      +ve      |        \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Component 1**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Members: ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes', 'Bagging', 'Adaboost']**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1mMat52.     \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mvariance   \u001b[0;0m  |  0.9720888366936934  |      +ve      |        \n",
      "  \u001b[1mlengthscale\u001b[0;0m  |   5.127609133763431  |      +ve      |        \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Component 2**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\r",
       "**Members: ['Linear SVM', 'KNN', 'Decision Trees', 'Perceptron', 'Logistic Regression', 'Gauss Naive Bayes', 'QDA', 'LDA']**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1mMat52.     \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mvariance   \u001b[0;0m  |   46.03231114719721  |      +ve      |        \n",
      "  \u001b[1mlengthscale\u001b[0;0m  |  21.029530765403038  |      +ve      |        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_score_single_pipeline': 0.06294085111764766,\n",
       " 'model_names_single_pipeline': '[ XGBoost ]',\n",
       " 'ensemble_score': 0.06402736930011581,\n",
       " 'ensemble_pipelines': ['[ XGBoost ]', '[ AdaBoost ]', '[ XGBoost ]'],\n",
       " 'ensemble_pipelines_weight': [0.337141036259983,\n",
       "  0.33191778877869144,\n",
       "  0.33094117496132563],\n",
       " 'optimisation_metric': 'aucprc',\n",
       " 'hyperparameter_properties': [{'name': 'XGBoost',\n",
       "   'hyperparameters': {'model': \"XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\\n       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\\n       importance_type='gain', interaction_constraints=None,\\n       learning_rate=0.06145542076570746, max_delta_step=0, max_depth=2,\\n       min_child_weight=1, missing=nan, monotone_constraints=None,\\n       n_estimators=253, n_jobs=0, num_parallel_tree=1,\\n       objective='binary:logistic', random_state=0, reg_alpha=0,\\n       reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\\n       validate_parameters=False, verbosity=None)\"}}],\n",
       " 'acquisition_type': 'MPI',\n",
       " 'kernel_members': {0: ['XGBoost',\n",
       "   'Gradient Boosting',\n",
       "   'Random Forest',\n",
       "   'Neural Network'],\n",
       "  1: ['Multinomial Naive Bayes',\n",
       "   'Bernoulli Naive Bayes',\n",
       "   'Bagging',\n",
       "   'Adaboost'],\n",
       "  2: ['Linear SVM',\n",
       "   'KNN',\n",
       "   'Decision Trees',\n",
       "   'Perceptron',\n",
       "   'Logistic Regression',\n",
       "   'Gauss Naive Bayes',\n",
       "   'QDA',\n",
       "   'LDA']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP_mdl.APReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "state": {
    "b36d11ca14b24a118b3c3a295a788faf": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
